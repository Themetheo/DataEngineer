# from pythainlp.tokenize import word_tokenize
# import time


# text = "‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡∏Å‡∏£‡∏≠‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÅ‡∏ñ‡∏° ‡∏ö‡∏∞‡∏´‡∏°‡∏µ‡πà‡∏ô‡πâ‡∏≥‡∏´‡∏°‡∏π‡∏™‡∏±‡∏ö‡∏´‡∏°‡∏π‡∏ï‡∏∏‡πã‡∏ô ‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡πÄ‡∏°‡∏•‡πå ‡∏Ç‡πâ‡∏≤‡∏ß‡∏Ç‡∏≤‡∏´‡∏°‡∏π"

# start_time = time.time()

# tokens = word_tokenize(text, engine="attacut")
# print("üìå Tokenized:", tokens)

# end_time = time.time()
# duration = end_time - start_time

# print(f"‚è±Ô∏è ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤: {duration:.4f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")

import time
from pythainlp.tokenize import word_tokenize
from tqdm import tqdm

# ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏ô‡∏π‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å
sample_texts = [
    "‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡∏Å‡∏£‡∏≠‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÅ‡∏ñ‡∏° ‡∏ö‡∏∞‡∏´‡∏°‡∏µ‡πà‡∏ô‡πâ‡∏≥‡∏´‡∏°‡∏π‡∏™‡∏±‡∏ö‡∏´‡∏°‡∏π‡∏ï‡∏∏‡πã‡∏ô",
    "‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡πÄ‡∏°‡∏•‡πå ‡∏Ç‡πâ‡∏≤‡∏ß‡∏Ç‡∏≤‡∏´‡∏°‡∏π‡∏û‡∏¥‡πÄ‡∏®‡∏©",
    "‡πÄ‡∏Å‡∏≤‡πÄ‡∏´‡∏•‡∏≤‡πÑ‡∏Å‡πà‡∏ï‡∏∏‡πã‡∏ô ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤",
    "‡∏Ç‡πâ‡∏≤‡∏ß‡∏°‡∏±‡∏ô‡πÑ‡∏Å‡πà‡∏ó‡∏≠‡∏î ‡πÅ‡∏ñ‡∏°‡∏ã‡∏∏‡∏õ","‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡∏Å‡∏£‡∏≠‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÅ‡∏ñ‡∏° ‡∏ö‡∏∞‡∏´‡∏°‡∏µ‡πà‡∏ô‡πâ‡∏≥‡∏´‡∏°‡∏π‡∏™‡∏±‡∏ö‡∏´‡∏°‡∏π‡∏ï‡∏∏‡πã‡∏ô ‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÄ‡∏£‡∏∑‡∏≠‡πÄ‡∏°‡∏•‡πå ‡∏Ç‡πâ‡∏≤‡∏ß‡∏Ç‡∏≤‡∏´‡∏°‡∏π",
] * 1000  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 4,000 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î

engine = "attacut"  # ‡∏´‡∏£‡∏∑‡∏≠ "newmm"

start_time = time.time()

for text in tqdm(sample_texts, desc=f"Testing engine: {engine}"):
    word_tokenize(text, engine=engine)

end_time = time.time()
duration = end_time - start_time

print(f"‚úÖ ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ tokenize ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ({engine}): {duration:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
print(f"üß† ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏ï‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {duration/len(sample_texts):.6f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ/‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°")
